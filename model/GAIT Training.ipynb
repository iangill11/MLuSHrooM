{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# data loading and processing\n",
    "import h5py\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set Script ID\n",
    "ID = 1\n",
    "\n",
    "# make directory for saving files\n",
    "dir_name = 'CVAE_{}'.format(ID)\n",
    "\n",
    "# Get the directory of the current script\n",
    "# get current directory\n",
    "\n",
    "script_dir = \"C:/Users/lukas/OneDrive - Yale University/Personal/Studies/Semester 3/S&DS 689/Project/MLuSHrooM/model/\"\n",
    "training_dir = os.path.join(script_dir, \"CVAE Training\")\n",
    "model_dir = os.path.join(training_dir, dir_name)\n",
    "\n",
    "# change directory to script directory\n",
    "os.chdir(script_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Set Hyperparameters \"\"\"\n",
    "# model training\n",
    "lr_dnn = 0.001\n",
    "num_epochs_dnn = 10000\n",
    "\n",
    "# model data\n",
    "batch_size_cvae = 32\n",
    "batch_size_dnn = 32\n",
    "input_dim = 64 # for square images\n",
    "latent_dim = 128\n",
    "\n",
    "\"\"\" Manual Parameters (Careful with these)\"\"\"\n",
    "N_train_batches = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data length:  1311\n",
      "Training data length:  1216\n",
      "Test data length:  95\n",
      "Data shape:  (1311, 64, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\AppData\\Local\\Temp\\ipykernel_8308\\3155500409.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  phi_data = torch.load('downscaled_data{}old.pt'.format(input_dim))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load Data \"\"\"\n",
    "# load data\n",
    "phi_data = torch.load('downscaled_data{}old.pt'.format(input_dim))\n",
    "\n",
    "# get training and test data lengths\n",
    "total_len = phi_data.shape[0]\n",
    "train_len = N_train_batches * batch_size_cvae\n",
    "test_len = total_len - train_len\n",
    "# print lengths\n",
    "print(\"Total data length: \", total_len)\n",
    "print(\"Training data length: \", train_len)\n",
    "print(\"Test data length: \", test_len)\n",
    "\n",
    "# print data shape\n",
    "print(\"Data shape: \", phi_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\AppData\\Local\\Temp\\ipykernel_8308\\1481131631.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('CVAE Training/CVAE_{}/CVAE_model_{}.pth'.format(ID, ID)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" Define and Load the CVAE model \"\"\"\n",
    "# Encoder Network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)  # 64x64 -> 32x32\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # 32x32 -> 16x16\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # 16x16 -> 16x16\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)  # 16x16 -> 8x8\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # 8x8 -> 8x8\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_mu = nn.Linear(256 * int((input_dim / 8))**2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * int((input_dim / 8))**2, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution -> batch normalization -> activation\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        \n",
    "        # Flatten before fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Compute mu and logvar for the latent space\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    \n",
    "# Decoder Network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Fully connected layer to project latent space to feature map\n",
    "        self.fc = nn.Linear(latent_dim, 256 * int((input_dim / 8))**2)\n",
    "        \n",
    "        # Deconvolutional layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 256, kernel_size=3, stride=1, padding=1)  # 8x8 -> 8x8\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # 8x8 -> 16x16\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1)   # 16x16 -> 16x16\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)    # 16x16 -> 32x32\n",
    "        self.deconv5 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)     # 32x32 -> 64x64\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Project latent space to feature map\n",
    "        z = F.relu(self.fc(z))\n",
    "        z = z.view(z.size(0), 256, int((input_dim / 8)), int((input_dim / 8)))  # Reshape to (batch_size, 256, 8, 8)\n",
    "        \n",
    "        # Apply deconvolution -> activation\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = F.relu(self.deconv2(z))\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        z = F.relu(self.deconv4(z))\n",
    "        return torch.sigmoid(self.deconv5(z))  # Output scaled between 0 and 1\n",
    "\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std) # std just gives dimension of tensor to give back\n",
    "        return mu + epsilon * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar, z # return reconstructed data, mu, logvar, and the latent state z\n",
    "    \n",
    "# Load the model\n",
    "model = CVAE(latent_dim=latent_dim)\n",
    "model.load_state_dict(torch.load('CVAE Training/CVAE_{}/CVAE_model_{}.pth'.format(ID, ID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi_train shape:  torch.Size([1216, 64, 64])\n",
      "phi_test shape:  torch.Size([95, 64, 64])\n",
      "Training data shape:  torch.Size([1216, 128])\n",
      "Test data shape:  torch.Size([95, 128])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Prepare Test and Train data for the DNN model \"\"\"\n",
    "\n",
    "# initialize empty tensors to store z-vectors\n",
    "z_train_data = torch.empty(train_len, latent_dim)\n",
    "z_test_data = torch.empty(test_len, latent_dim)\n",
    "\n",
    "# split phi_train and phi_test data\n",
    "phi_train = torch.tensor(phi_data[:train_len])\n",
    "phi_test = torch.tensor(phi_data[train_len:])\n",
    "\n",
    "print(\"phi_train shape: \", phi_train.shape)\n",
    "print(\"phi_test shape: \", phi_test.shape)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(train_len):\n",
    "        input = phi_train[i].unsqueeze(0).unsqueeze(0)\n",
    "        z_train_data[i] = model.encoder(input)[0]\n",
    "        \n",
    "    for i in range(test_len):\n",
    "        input = phi_test[i].unsqueeze(0).unsqueeze(0)\n",
    "        z_test_data[i] = model.encoder(input)[0]\n",
    "\n",
    "# print shapes of data\n",
    "print(\"Training data shape: \", z_train_data.shape)\n",
    "print(\"Test data shape: \", z_test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class for dataset\n",
    "class ZDataset(Dataset):\n",
    "    def __init__(self, z_data):\n",
    "        self.data = z_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data.shape[0] - 1) # the sataset consists of samples and targets, but there are only n-1 targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.data[idx + 1]\n",
    "        return sample, target\n",
    "    \n",
    "# train data loader\n",
    "z_train = (z_train_data)\n",
    "z_train_loader = DataLoader(z_train, batch_size=batch_size_dnn, shuffle=True)\n",
    "\n",
    "# test data loader\n",
    "z_test = ZDataset(z_test_data)\n",
    "z_test_loader = DataLoader(z_test, batch_size=batch_size_dnn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define the DNN model \"\"\"\n",
    "# DNN Model\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)\n",
    "\n",
    "# define loss function for DNN\n",
    "def loss_function_dnn(output, z):\n",
    "    return F.mse_loss(output, z, reduction='sum')\n",
    "\n",
    "\n",
    "\"\"\" Train the DNN model \"\"\"\n",
    "os.chdir(model_dir) # go to model directory to save files\n",
    "\n",
    "dnnmodel = DNN(latent_dim)\n",
    "optimizer = optim.Adam(dnnmodel.parameters(), lr=lr_dnn)\n",
    "\n",
    "# create loss list to visualize loss\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs_dnn):\n",
    "    dnnmodel.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(z_train_loader): # get sample and target batches\n",
    "        sample, target = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = dnnmodel(sample)\n",
    "        loss = loss_function_dnn(output, target)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = train_loss / len(z_train_loader.dataset)\n",
    "    # open file to write loss\n",
    "    with open('DNN_loss_{}.txt'.format(ID), 'a') as f:\n",
    "        f.write('{},{}\\n'.format(epoch, avg_loss))\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "# plot loss and save image\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(losses, color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('DNN Loss')\n",
    "plt.grid()\n",
    "plt.savefig('DNN_loss{}.png'.format(ID))\n",
    "\n",
    "# save the model\n",
    "torch.save(dnnmodel.state_dict(), 'DNN_model_{}.pth'.format(ID))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
